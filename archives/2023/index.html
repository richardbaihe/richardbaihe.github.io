<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>Richard's website</title><meta name="author" content="He Bai (Richard)"><link rel="shortcut icon" href="/img/favicon.png"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.13.0/css/all.min.css"><meta name="generator" content="Hexo 5.4.2"></head><body><header id="page_header"><div class="header_wrap"><div id="blog_name"><a class="blog_title" id="site-name" href="/">Richard's website</a></div><button class="menus_icon"><div class="navicon"></div></button><ul class="menus_items"><li class="menus_item"><a class="site-page" href="/#About"> About Me</a></li><li class="menus_item"><a class="site-page" href="/#Publications"> Publications</a></li></ul></div></header><main id="page_main"><div class="side-card sticky"><div class="card-wrap" itemscope itemtype="http://schema.org/Person"><div class="author-avatar"><img class="avatar-img" src="/img/avatar.jpeg" onerror="this.onerror=null;this.src='/img/profile.png'" alt="avatar"></div><div class="author-discrip"><h3>He Bai (Richard)</h3><p class="author-bio"></p></div><div class="author-links"><button class="btn m-social-links">Links</button><ul class="social-icons"><li><a class="social-icon" href="https://twitter.com/richard_baihe" target="_blank"><i class="fab fa-twitter" aria-hidden="true"></i></a></li><li><a class="social-icon" href="https://www.linkedin.com/in/he-bai-5356b1142/" target="_blank"><i class="fab fa-linkedin" aria-hidden="true"></i></a></li><li><a class="social-icon" href="mailto:[he.bai@uwaterloo.ca]" target="_blank"><i class="fas fa-envelope" aria-hidden="true"></i></a></li></ul><ul class="social-links"><li><a class="e-social-link" href="https://scholar.google.com/citations?user=MIcmEaMAAAAJ" target="_blank"><i class="fas fa-graduation-cap" aria-hidden="true"></i><span>Google Scholar</span></a></li><li><a class="e-social-link" href="https://github.com/richardbaihe" target="_blank"><i class="fab fa-github" aria-hidden="true"></i><span>Github</span></a></li></ul></div><a class="cv-links" href="/attaches/cv_tex/resume_HeBai.pdf" target="_blank"><i class="fas fa-file-pdf" aria-hidden="true"><span>My Detail CV.</span></i></a></div></div><div class="page" itemscope itemtype="http://schema.org/CreativeWork"><article><h1 id="About"><a href="#About" class="headerlink" title="About"></a>About</h1><p>I am a Machine Learning Research Scientist at <a target="_blank" rel="noopener" href="https://machinelearning.apple.com/">Apple MLR</a>, primarily working on natural language processing and large model pretraining.</p>
<p>I received my PhD degree from the University of Waterloo, worked on language modeling and unsupervised machine learning under the supervision of <a target="_blank" rel="noopener" href="https://scholar.google.ca/citations?user=oGgPXFEAAAAJ&hl=en">Ming Li</a>.<br>Before that, I worked with <a target="_blank" rel="noopener" href="https://scholar.google.com.hk/citations?user=l8lvKOQAAAAJ&hl=en">Chengqing Zong</a> on spoken language understanding.</p>
<p>I have served as a PC member of ACL (2020-2024), EMNLP (2019-2023), ICML (2022-2023), Neurips (2023), ICLR (2023-2024), AAAI (2020), COLING (2020-2024). I received ICML Outstanding Reviewers awards (2022). I am an organizer of Embodied AI Workshop in CVPR 2024.</p>
<!-- 
I am one of the challenge organizers of Embodied AI Workshop in CVPR 2024.  -->

<p>My recent research focuses on below topics:</p>
<ul>
<li>Long-form sequence modeling</li>
<li>LLM Factualness and Evaluation</li>
<li>Multilingual NLP</li>
</ul>
<!-- 
My past works concern modeling text and speech sequences to achieve lower perplexity, better generation, and benefit downstream language tasks; specifically, we address the problem of modeling text and text-speech sequences with Transformer-based language models. My favorite works during my Ph.D. study are [Segment-Aware Language Modeling](https://arxiv.org/abs/2004.14996), [Hypernym-Instructed Language Modeling](arxiv.org/abs/2203.10692), and [Alignment-Aware Acoustic and Text Modeling](arxiv.org/abs/2203.09690).  -->

<h1 id="Selected-Publications"><a href="#Selected-Publications" class="headerlink" title="Selected Publications"></a>Selected Publications</h1><p>Y Zhang*, *<em>H Bai**</em>, R Zhang*, J Gu, S Zhai, J Susskind, N Jaitly. How Far Are We from Intelligent Visual Deductive Reasoning? arXiv preprint arXiv:2403.04732. 2024. (*equal)</p>
<p>Z Wu, <strong>H Bai</strong>, A Zhang, J Gu, VG Vydviswaran, N Jaitly, Y Zhang. Divide-or-Conquer? Which Part Should You Distill Your LLM? arXiv preprint arXiv:2402.15000. 2024.</p>
<p>P Maini, S Seto, <strong>H Bai</strong>, D Grangier, Y Zhang, N Jaitly. Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling. arXiv preprint arXiv:2401.16380. 2024.</p>
<p>S Zheng*, *<em>H Bai**</em>, Y Zhang, Y Su, X Niu, N Jaitly. KGLens: A Parameterized Knowledge Graph Solution to Assess What an LLM Does and Doesn’t Know. arXiv preprint arXiv:2312.11539. 2023. (*equal)</p>
<p>A Mousavi, X Zhan, <strong>H Bai</strong>, P Shi, T Rekatsinas, B Han, Y Li, J Pound, … Construction of Paired Knowledge Graph-Text Datasets Informed by Cyclic Evaluation. arXiv preprint arXiv:2309.11669. 2023.</p>
<p><strong>H Bai</strong>. Novel Methods for Natural Language Modeling and Pretraining. University of Waterloo. 2023.</p>
<p>P Shi, L Song, L Jin, H Mi, <strong>H Bai</strong>, J Lin, D Yu. Cross-lingual Text-to-SQL Semantic Parsing with Representation Mixup. Findings of the Association for Computational Linguistics: EMNLP 2022, 5296-5306. 2022.</p>
<p>Xiaoran Fan, Chao Pang, Tian Yuan, <strong>He Bai</strong>, Renjie Zheng, Pengfei Zhu, Shuohuan Wang, Junkun Chen, Zeyu Chen, Liang Huang, Yu Sun, Hua Wu. ERNIE-SAT: Speech and Text Joint Pretraining for Cross-Lingual Multi-Speaker Text-to-Speech. (preprint) <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2211.03545">[pdf]</a><a href="https://github.com/PaddlePaddle/PaddleSpeech/tree/develop/examples/aishell3_vctk/ernie_sat">[code]</a></p>
<p>Peng Shi, Rui Zhang, <strong>He Bai</strong>, Jimmy Lin. XRICL: Cross-lingual Retrieval-Augmented In-Context Learning for Cross-lingual Text-to-SQL Semantic Parsing. Finding of EMNLP 2022. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2210.13693">[pdf]</a><a href="https://github.com/Impavidity/XRICL">[code]</a></p>
<p><strong>He Bai</strong>, Renjie Zheng, Junkun Chen, Xintong Li, Mingbo Ma, Liang Huang. A3T: Alignment-Aware Acoustic and Text Pretraining for Speech Synthesis and Editing.  ICML 2022 (full paper) <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.09690">[pdf]</a><a href="https://github.com/richardbaihe/a3t">[code]</a>.</p>
<p><strong>He Bai</strong>, Tong Wang, Alessandro Sordoni, Peng Shi. Better Language Model with Hypernym Class Prediction. ACL 2022 (full paper) <a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=YjZH6EpuSY">[pdf]</a> <a href="https://github.com/richardbaihe/robustLM">[code]</a>.</p>
<p>Peng Shi, Rui Zhang, <strong>He Bai</strong>, Jimmy Lin. Cross-Lingual Training with Dense Retrieval for Document Retrieval. EMNLP-MSR 2021 (workshop paper) <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2109.01628.pdf">[pdf]</a>.</p>
<p><strong>He Bai</strong>, Peng Shi, Jimmy Lin, Luchen Tan, Kun Xiong, Wen Gao, Jie Liu, Ming Li. Semantics of the Unwritten: The Effect of End of Paragraph and Sequence Tokens on Text Generation. ACL-SRW 2021 (workshop paper) <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2004.02251.pdf">[pdf]</a> <a href="https://github.com/rsvp-ai/semantic_unwritten">[code]</a>.</p>
<p><strong>He Bai</strong>, Peng Shi, Jimmy Lin, Yuqing Xie, Luchen Tan, Kun Xiong, Wen Gao, Ming Li. Segatron: Segment-awareTransformer for Language Modeling and Understanding. AAAI 2021. (full paper) <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2004.14996">[pdf]</a> <a href="https://github.com/rsvp-ai/segatron_aaai">[code]</a></p>
<p>Peng Shi, <strong>He Bai</strong>, Jimmy Lin. Cross-Lingual Training of Neural Models for Document Ranking. EMNLP Findings 2020. (short paper) <a target="_blank" rel="noopener" href="https://www.aclweb.org/anthology/2020.findings-emnlp.249/">[pdf]</a> <a href="https://github.com/Impavidity/cross-lingual-doc-ranking">[code]</a></p>
<p><strong>He Bai</strong>, Yu Zhou, Jiajun Zhang and Chengqing Zong. Memory Consolidation for Contextual Spoken Language Understanding with Dialogue Logistic Inference. ACL 2019. (short paper) <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1906.01788.pdf">[pdf]</a> <a href="https://github.com/richardbaihe/conslu">[code]</a></p>
<p><strong>He Bai</strong>, Yu Zhou, Jiajun Zhang, Liang Zhao, Mei-Yuh Hwang and Chengqing Zong. Source Critical Reinforcement Learning for Transferring Spoken Language Understanding to a New Language. COLING 2018. (full paper) <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1808.06167.pdf">[pdf]</a> </p>
</article></div></main><div class="nav-wrap"><div class="nav"><button class="site-nav"><div class="navicon"></div></button><ul class="nav_items"><li class="nav_item"><a class="nav-page" href="/#About"> About Me</a></li><li class="nav_item"><a class="nav-page" href="/#Publications"> Publications</a></li></ul></div><div class="cd-top"><i class="fa fa-arrow-up" aria-hidden="true"></i></div></div><footer id="page_footer"><div class="footer_wrap"><div class="copyright">&copy;2020 - 2024 by He Bai (Richard)</div><div class="theme-info">Powered by <a target="_blank" href="https://hexo.io" rel="nofollow noopener">Hexo</a> & <a href="https://github.com/PhosphorW/hexo-theme-academia" rel="nofollow">Academia Theme</a></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery-pjax@latest/jquery.pjax.min.js"></script><script src="/js/main.js"></script></body></html>